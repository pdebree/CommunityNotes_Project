---
title: "data_cleaning"
output: html_document
date: "2025-11-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Cleaning File 

This .Rmd file contains the data cleaning necessary for our project.

### Library Imports
```{r}
library(tidyverse)
# For language input
library(ellmer)
library(cld2)
library(stringr)
library(openai)
library(data.table)
library(stm)
library(MASS)
library(tm)
library(topicmodels)
library(tidytext)
library(keyATM)
library(quanteda)
```


### Data Imports

The data, from the X website is updated daily. Meaning that can have data to this point that has 

The notes can be found at the https://communitynotes.x.com/guide/en/under-the-hood/download-data, 
however it should be noted that only users with X accounts can download the data. 

A good description can be found at the same website.

As an overview the data takes on the following form: 
- `notes` contains information about a note generally and how it can be 


# Description of Data 
Data Sources, date and where they are located.

Mention the difference when working with the ratings datasets 
Script for reading in the ratings - reliant on the valid notes because of the 
amount of data we are processing 

# Data Load in 
Describe dataset below and why we aren't doing the ratings processing yet. 

```{r}
# Load in the notes data, the notes status history, the user enrollment and abat Signals 
# make sure the noteId is read in as a string
notes <- read_tsv("data/notes-00000.tsv", col_types=cols(noteId=col_character()))
notes_status_history <- read_tsv("data/noteStatusHistory-00000.tsv", col_types=cols(noteId=col_character()))
user_enrollment_status <- read_tsv("data/userEnrollment-00000.tsv")
bat_signals <- read_tsv("data/batSignals-00000.tsv")

```

We had to specify column type for the `noteId` explicitly, because we found that when we read in the ratings and notes they had different types, causing some problems when we combined these. 

# Make the times interpretable.

We also add `month_year` variables for the `notes` and `ratings` data. 

We only do one ratings data frame (of the 20) here. 

Within the custom make_notes cleaning functions below, each data frame has their, relevant, time variables altered so that they include a `month_year` variable.

```{r}
source('cleaning.R')
notes <- make_notes_times(notes)
notes_status_history <- make_notes_history_times(notes_status_history)
user_enrollment_status <- make_user_enrollment_status_times(user_enrollment_status)
bat_signals <- make_bat_signals_times(bat_signals)
```


# Data Description: 

The data was taken from https://x.com/i/communitynotes/download-data (note: downloading this data does require an X account) on October 16th 2025. This means that the data we use are the community notes up to that date. 


# Drop NA

In our exploration of the data we focused primarily on the form of the notes data frame. As our primary data it is pertinent that we focus on get a valid subset of the data in order to ensure that our modeling will not be influence by notes that cannot be relied. 

Let's look at the notes missing values more generally. First, it is 
helpful to look at the number of missing values from each column. The code below
does this.

```{r}
colSums(is.na(notes))
```


Cleaning Notes Further 
- We found 2 notes with missing text values - we drop them
- We found 4 rows that are duplicates of other rows, so drop all 8 of these to 
ensure no confusion (These are all classified as MISINFORMED_OR_POTENTIALLY_MISLEADING, however the rows are not exact duplicates. Even the text is different. Because of this confusion we drop the rows).
    
There are also 8 duplicates in notes status history - and they all have the NEEDS_MORE_RATINGS status, so we easily drop these rows. 


In the `clean_notes()` function called below a few things happen
- The 2 missing notes are removed, 8 duplicated noteId values are removed - both the original version and the duplicate are remove. Finally, our exploration found that there are a few metrics that are were used in the very start of Community Notes 

The code below outputs how missing values in the dataset behave over the `month_year` combinations. This gave us a good idea of how the data actually formed.

```{r}
print(notes %>% group_by(month_year) %>% 
  summarize(harmful_na=sum(is.na(harmful)),
            harmful_not_na=sum(!is.na(harmful)), 
            believable_na=sum(is.na(believable)),
            believable_not_na=sum(!is.na(believable)),
            validationDifficulty_na=sum(is.na(validationDifficulty)),
            validationDifficulty_not_na=sum(!is.na(validationDifficulty))), n=Inf, width=Inf)
```


From this, we found that the exact same number of pieces of missing data came
from three attributes: `harmful`, `believable`, `validationDifficulty`. There 
seems to be systematic discrepancy here and, again, looking at time will be 
helpful. We can just look at one of these three values in order to see the
number of missing values. Let's similarly count the number of missing values
for each `month_year` value to see if there is an identifiable trend. 

The output above shows that there are only non-missing values for `harmful`, `believeable` and `validationDifficulty` before the `2022-11` month. As we are going to limit our data to only notes after Community Notes went global, we just remove these rows entirely in the cleaning. 

`clean_bat_signals()` limits the `tweetId` values to those that are 19 character long (as they should be) and makes a list of sources for each tweet. When a bat signal is sent (i.e. a user requests a community note on a tweet) they can supply a url of a website that contains information that may be relevant to a community note. As there can be multiple bat signals for a tweet, we created lists of source links. Though this is not used in this project, it may be helpful moving forward. 


```{r}
source("cleaning.R")
notes <- clean_notes(notes)
notes_status_history <- clean_notes_history(notes_status_history)
bat_signals <- clean_bat_signals(bat_signals)
```


Let's look a bit into the validity of each note as being helpful or not. 

Below we look at how many notes never recieved their first Non-NMR status. This is an indicator of how many notes never gained a enough ratings to be considered `helpful` or `notHelpful`. A major potential flaw in the system is that there could simply not be enough people using Community Notes - this is a major motivator for implementing an LLM to do the work of rating notes. 

```{r}
# all notes
table(notes_status_history$firstNonNMRStatus, useNA="ifany")

# english notes
table(notes_status_history %>% filter(noteId %in% (limit_english(notes) %>% pull(noteId))) %>% pull(firstNonNMRStatus), useNA="ifany")

```

From all of the data 
259194 are `CURRENTLY_RATED_HELPFUL`
152090 are `CURRENTLY_RATED_NOT_HELPFUL`
1902300 do not have NON-NMR status 


Then in only English: 
134683 are `CURRENTLY_RATED_HELPFUL`
76487 are `CURRENTLY_RATED_NOT_HELPFUL`
1045958 do not have NON-NMR status

There is likely something to be said about the validity of content moderation in different languages. 

There is a difference between a firstNonNMRStatus and LockedStatus. Looking at firstNONNMRStatus let's us get an idea of how many notes never get enough ratings to change status. Looking at LockedStatus let's us see how may notes get a final decision on their helpfulness.

```{r}
table(notes_status_history$lockedStatus, useNA="ifany")

table(notes_status_history %>% filter(noteId %in% (limit_english(notes) %>% pull(noteId))) %>% pull(lockedStatus), useNA="ifany")

```


From all of the data 
177873 are `CURRENTLY_RATED_HELPFUL`
93155 are `CURRENTLY_RATED_NOT_HELPFUL`
1858642 are `NEEDS_MORE_RATINGS`
183914 are `NA`


Then in only English: 
99019 are `CURRENTLY_RATED_HELPFUL`
45941 are `CURRENTLY_RATED_NOT_HELPFUL`
1039682 are `NEEDS_MORE_RATINGS`
72486 are `NA`

It is really surprising how many NA values are here - we struggled to get an understanding from the documentation why this is. Intuitively, it seems that it would make sense for there to be some reason, or inherent coding, as to why there are so many missing values. 


# Here we can do some analysis of notes over time. 

We could have done this in our initial cleaning, but felt that looking a bit into trends over time was helpful.

Limit time to relevant area - from when it went global (In November 2022) to September 2025 (get a full month). This does not get rid of too much data because there really wasn't much use before March 2023.

As we have said from the exploration above, there is a demarcation with when community notes became really actively used. And we only have data that goes up to October 16th 2025. (Relevant that the ceasefire was on October 10th 2025)


Now that we have month_year combinations we can look a bit into how the number of notes and ratings have changed over time. It is also interesting to see how many ratings per note there are. 

```{r}
ggplot(data = notes, aes(x = month_year)) +
  geom_bar() + 
  scale_x_discrete(

    breaks = function(x) {
      x[grepl("-01", x) | grepl("-07", x)]
    }
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x="Date of Note Creation (in Months)", y="Number of Notes Created")
```


From looking into this data, we think it makes sense to look at a time frame where the platform was being consistently used. Community Notes was initially launched only in the US, in January 2021 but was introduced globally on December 12th 2022. However, it didn't really pick up until March 2023 (the total count of notes went from 5000 to 15000 from February to April). We therefore decide to limit our valid notes to those from November 2022 to, and including September 2025. 



# Getting Valid Notes

Once we have our cleaned data, we can begin to think about what makes a note valid for our analysis. In `get_valid_notes()`, the `notes` and `notes_status_history` data frames are combined. This is because we define a note that is valid for analysis as one that has a `lockedStatus` of either `CURRENTLY_RATED_HELPFUL` or `CURRENTLY_RATED_NOT_HELPFUL`. The locked status comes when there has been no change in the note for 2 weeks. 


```{r}
valid_notes <- get_valid_notes(notes, notes_status_history)
english_valid_notes <- limit_english(valid_notes)

# Additional code for potential tweet note integration
#notes_w_sourcelink <- left_join(notes, batSignals_complete, by="tweetId")

```

# Ratings Read in 

There is a lot of ratings data, so we process it on import to ensure that we only have the necessary data loaded in. This is based on our time frame, though we do not limit ratings to September 2025 - we allow ratings from October 2025 on notes written in September 2025 to be part of this data set. However, because there are so many notes we limit the ratings to those that have a `noteId` in our valid subset. This does mean that we do not look at all of the data but, we are only looking at ratings that have a meaning in our analysis.


```{r}
# pull only english valid note ids
english_valid_notes_ids <- english_valid_notes %>% pull(noteId)

# Read in the ratings 
ratings_valid <- read_in_ratings(english_valid_notes_ids)
```

Now we have the data that we believe to be valid in our exploration, we can look at how it is spread over time. 
```{r}
ggplot(data = valid_notes, aes(x = month_year)) +
  geom_bar() + 
  scale_x_discrete(

    breaks = function(x) {
      x[grepl("-01", x) | grepl("-07", x)]
    }
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x="Date of Note Creation (in Months)", y="Number of Notes Created")

ggplot(data = ratings_valid, aes(x = month_year)) +
  geom_bar() + 
  scale_x_discrete(

    breaks = function(x) {
      x[grepl("-01", x) | grepl("-07", x)]
    }
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x="Date of Note Creation (in Months)", y="Number of Notes Created")

```

These plots both show that the notes and ratings are fairly well spread out over the time period. Just to reiterate, the ratings data period is one month longer than the notes time period. This allows for a set of valid ratings on a these final notes (from September 2025).


Now let's look a bit into notes and ratings combinations. We should find that every note in our valid dataset has at least 10 ratings. If not, it is not possible for the note to have obtained a `LockedStatus`. 

```{r}
# Create counts for how many ratings each note has 
ratings_note_counts <- ratings_valid %>% group_by(noteId) %>% summarize(number_of_ratings=n())

notes_w_counts <- left_join(valid_notes, ratings_note_counts, by="noteId")

# Only looked at notes with less than 100 ratings for interpretability. There was one note with 22272 ratings. We lose 1,887 notes from this visualization
ggplot(data=notes_w_counts %>% filter(number_of_ratings < 100), aes(x=number_of_ratings)) + geom_bar() + labs(x="Number of Ratings for One Note", y="Count of Number of Ratings")
```

We can see that there are some notes still with less than 10 ratings. To ensure the validity of our modeling we will limit our valid notes further to only those with at least 10 corresponding ratings in our data set. 

```{r}
valid_notes_by_count_id <- notes_w_counts %>% filter(number_of_ratings >= 10) %>% pull(noteId)

# remove the invalid notes from the notes data frame and ratings for these notes from the
# ratings frame
valid_notes <- valid_notes %>% filter(noteId %in% valid_notes_by_count_id)
valid_ratings <- ratings_valid %>% filter(noteId %in% valid_notes_by_count_id)
```

Let's look at some summary statistics on how many ratings there are per note.

```{r}
valid_notes_combined <- left_join(valid_notes, valid_ratings %>% group_by(noteId) %>% summarize(number_of_ratings=n()), by="noteId")
summary(valid_notes_combined$number_of_ratings)
```

This is pretty severely right skewed. 

```{r}
write.csv(valid_notes, "data/valid_notes.csv", row.names=FALSE)
write.csv(valid_ratings, "data/valid_ratings.csv", row.names=FALSE)
```


Finally, we have 138,884 notes and 20,427,356 ratings.

We have lost a lot of data in this process. Starting with 2,153,194 notes and ending with 138,884. A large portion of this missing data does come from notes that could not be identified as English. 

