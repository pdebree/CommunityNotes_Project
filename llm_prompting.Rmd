---
title: "prompting"
output: html_document
date: "2025-11-20"
---

# Prompting Grok 

```{r}
library(tidyverse)
library(httr2)
library(jsonlite)
library(usethis)
library(data.table)
library(tidyverse)
library(stringr)
# R script with prompting functions
source("grok_notes_prompting.R")
```

# Read in data 

We use the valid notes generated from our data cleaning process (found in `data_cleaning.Rmd`), as well as the notes with the topics Gaza and Ukraine - based on the Topic Modeling we performed in `topic_selection.Rmd`. 

```{r}
# Read in Valid English Notes (by our previous definition fo what makes a note good)
notes_valid_in <- read_csv("data/topic_valid_notes.csv", col_types=cols(noteId=col_character()))
```

We - based on the topic modeling - get llm outputs for the notes from the Gaza and Ukraine sets. 


```{r}
gzuk_notes <- notes_valid_in %>% filter(topic == "1_Ukraine" | topic == "2_Gaza")
```


We have 23876 notes. However, we want to keep a large portion of these in our training and test set. So we will aim for about 1000 notes. 

Below we create batches of data, group by 10, for API calls. Before doing this we shuffled all of the relevant Gaza and Ukraine data, so that any amount that we generated notes for would be a representative sample of the Gaza and Ukraine data. 

```{r}
# Make into a table and shuffle (to randomize)
# We randomized in order to make sure that if we did not get all the ratings from the llm, the ones we did are good enough 
gzuk_notes <- as.data.table(gaza_ukraine_notes) 
gzuk_notes <- gzuk_notes[sample(1000)]

# pull out the note text from the subgroups 
get_notes <- function(notes_data) {
  return(notes_data[["note_text"]])
}

gzuk_text <- get_notes(gzuk_notes)
group_size <- 10

# make groups of size 10 to prompt grok with. 
group_texts <- gl(
  n = ceiling(length(gaza_ukraine_text) / group_size), 
  k = group_size, 
  length = length(gaza_ukraine_text)
)

# We will just use the first 10 groups of 200 
gzuk_texts_split <- split(gaza_ukraine_text, group_texts)
gzuk_ids_split <- split(gzuk_notes$noteId, group_texts)
```

# Make attributes ready:

After looking at some of the data, we only have some columns that are helpful. Ultimately, we are most focused on the helpfulnessLevel attributes - but since we are limited by not having the tweet data it is helpful to have more nuanced inputs.


# Talk about why we chose which ratings parameters (could not include those that have are really reliant on the tweet, because we don't have that text)
These are the rating attributes we have removed: "helpfulAddressesClaim", "helpfulImportantContext","notHelpfulMissingKeyPoints","notHelpfulIrrelevantSources",  
"notHelpfulNoteNotNeeded"

The Grok API key is loaded in as a system variable. We use grok-3 because this 
was the most cost effective way to do our prompt engineering. 

Load in model parameters for prompting: 
```{r}
grok_api_key <- Sys.getenv("GROK_API_KEY")
grok_model <- "grok-3"
grok_url <- "https://api.x.ai/v1/chat/completions"
system_prompt <- paste(
  "You are an expert evaluator of X's Community Notes. Your task is to analyze the provided note's text, and generate a set of ratings based purely on the note's quality of information",
  "---",
  "**CRITICAL CONSTRAINT:** You must set **exactly one** of the three helpfulness level fields (`helpfulnessLevel_helpful`, `helpfulnessLevel_somewhathelpful`, or `helpfulnessLevel_nothelpful`) to `TRUE, and the other two to `FALSE`.",
  "**CRITICAL CONSTRAINT:** For the other 9 fields, set them to `TRUE` if the characteristic applies to the note, and `FALSE` if it does not. The must be named exactly as follows: `helpfulClear`, `helpfulGoodSources`, `helpfulUnbiasedLanguage`, `notHelpfulIncorrect`,`notHelpfulSourcesMissingOrUnreliable`, `notHelpfulHardToUnderstand`, `notHelpfulArgumentativeOrBiased`, `notHelpfulSpamHarassmentOrAbuse`, `notHelpfulOpinionSpeculation",
  "---",
  "**CRITICAL CONSTRAINT:** Your response MUST be a JSON object that strictly adheres to the provided JSON Schema. The key values of the JSON Schema for every note MUST be the same as the JSON schema input. Do not include any text, reasoning, or formatting outside of the JSON object. There must be 50 outputs, do NOT nest the 12 fields within each note. The JSON format must not have any nesting at all and should be easy to convert to a data frame.
  e.g. \"note1_helpfulnessLevel_helpful\": true,\n  \"note1_helpfulnessLevel_somewhathelpful\": false,\n  \"note1_helpfulnessLevel_nothelpful\": false,\n  \"note1_helpfulClear\": true,\n  \"note1_helpfulGoodSources\": true,\n  \"note1_helpfulUnbiasedLanguage\": true,\n  \"note1_notHelpfulIncorrect\": false,\n  \"note1_notHelpfulSourcesMissingOrUnreliable\": false,\n  \"note1_notHelpfulHardToUnderstand\": false,\n  \"note1_notHelpfulArgumentativeOrBiased\": false,\n  \"note1_notHelpfulSpamHarassmentOrAbuse\": false,\n  \"note1_notHelpfulOpinionSpeculation\": false,\n\n")
```

We had to iterate over quite a few versions of this system prompt before we got to a point that we were happy with. This was an unexpectedly large amount of effort and took us quite a while to figure out. There are probably upwards of 10 versions that we tried, and then once we tried we had to see how well the prompt scaled up. This was another issue. 


User prompt looks like this: 
"**TEXT:** ", text, "\n\n",
"Generate the structured rating in the required format."))}

# Create the Ratings 

The code for the actual api calls can be found in `grok_notes_prompting.R` but below 
we perform the batched api calls. We decided to do this here and not in the script 
because processing the generated data required quite a bit of effort and manual adjustments. 


```{r}
# Make the JSON schema for our LLM prompt output. 
json_schema <- make_json_schema()

# create a list to hold the output of our llm prompts. 
grok_ratings <- list()

for (i in 1:100) {
  
  # create messages list
  notes_messages <- make_request_message(gaza_ukraine_texts_split[[i]])
   
  # perform api request. 
  grok_ratings_request_output <- perform_request(messages=notes_messages, notes_json_schema=json_schema)

  # pull out content from response, of JSON output
  json_data_string <- resp_body_json(grok_ratings_request_output)$choices[[1]]$message$content[1]
  
  # save JSON output to list of ratings.
  grok_ratings[[i]] <- json_data_string
  print("round complete")
  print(i)
}
```

Below we combine the grok outputs into one data frame, ready for modeling. 

```{r}
grok_frames <- lapply(grok_ratings_browser, format_grok_to_dataframe)
valid_grok_ratings_frame <- bind_rows(grok_frames)
valid_grok_ratings_frame$Note <- unname(unlist(gzuk_ids_split))[1:560]
valid_grok_ratings_frame <- valid_grok_ratings_frame %>% rename(noteId=Note)

# save grok outputs 
write.csv(valid_grok_ratings, "data/valid_grok_ratings_560.csv", row.names=FALSE)
```
