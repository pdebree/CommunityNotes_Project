---
title: "ratings_processing"
output: html_document
date: "2025-11-24"
---

```{r}
library(tidyverse)
source("cleaning.R")
library(data.table)
```



```{r}
english_valid_notes <- read.csv("data/english_valid_notes.csv")
valid_notes_ids <- english_valid_notes %>% pull(noteId)
```





# Code to create average ratings across ratings dataset

Need to find the average rating for all users - for noteIds. Then we either use this average ratings (or we push the values to 0 and 1 - to make the outputs more like the llm outputs we will get). 

The values we will need to process are as follows: 


Future work could include doing more complex modeling of the variation in the raters. 


Check that agree and disagree are completely mutually exclusive - both of these have 0 values so we just remove them 

Go rid of the following variables: 
- `agree`, `disagree`, `helpful`, `ratingSourceBucketed`, `version`, `notHelpful`, `helpfulOther` (this is just like a helpfulness without distinction mark - not really useful), `helpfulInformative` 

"notHelpfulOpinionSpeculationOrBias"  

We should get something like the helpfuls all add up (i.e. mutually exclusive multiple choice?)



Keep following variables: 
- `Help
```{r}
ratings_value_variables <- c("helpfulnessLevel_helpful", "helpfulnessLevel_somewhathelpful", "helpfulnessLevel_nothelpful", "helpfulClear", "helpfulGoodSources", "helpfulAddressesClaim", "helpfulImportantContext", "helpfulUnbiasedLanguage", "notHelpfulIncorrect", "notHelpfulSourcesMissingOrUnreliable", "notHelpfulMissingKeyPoints", "notHelpfulHardToUnderstand", "notHelpfulArgumentativeOrBiased", "notHelpfulSpamHarassmentOrAbuse", "notHelpfulIrrelevantSources", "notHelpfulOpinionSpeculation", "notHelpfulNoteNotNeeded")    

ratings_logisitic_variables <- c("noteId", "raterParticipantId", "time_created","month_year")

# Find the means of these columns, but only in their noteId groups (this gives us an average of the sentiments for each noteId)
mean_ratings <- ratings_valid %>% mutate(
  helpfulnessLevel_helpful = (helpfulnessLevel == "HELPFUL"),
  helpfulnessLevel_somewhathelpful = (helpfulnessLevel == "SOMEWHAT_HELPFUL"),
  helpfulnessLevel_nothelpful= (helpfulnessLevel == "NOT_HELPFUL")) %>% 
    select(all_of(ratings_value_variables), all_of(ratings_logisitic_variables)) %>% 
  group_by(noteId) %>% summarize(across(all_of(ratings_value_variables), mean, na.rm = TRUE)
  
```




```{r}
# Modeling variables 
notes_modeling_variables <- c("lockedStatus", "ukraine_conflict", "gaza_conflict", "messi_ronaldo", "scams", "noteId")

# Add in locked status 
modeling_ratings <- left_join(mean_ratings, notes_valid %>% select(all_of(notes_modeling_variables)), by="noteId") %>% mutate(rated_helpful=(lockedStatus=="CURRENTLY_RATED_HELPFUL"))
```


Intuition is that a random forest will do better because the form of the data. We have subgroups within the helpfulness levels.

(Question - are we as concerned with multi collinarity with a random forest?)



```{r}
first_fit <- lm(rated_helpful ~ helpfulnessLevel_helpful,data=modeling_ratings)

```








