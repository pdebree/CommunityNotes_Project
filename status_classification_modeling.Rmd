---
title: "status_classification_modeling.Rmd"
output: html_document
date: "2025-12-05"
---

```{r}
library(tidyverse)
library(ranger)
library(ROCR)
source("classification.R")
```



Read in the valid notes data, with topic markers, the ratings data and the generated LLM outputs

```{r}
notes <- read_csv("data/topic_valid_notes.csv", col_types=cols(noteId=col_character()))
ratings <- read_csv("data/valid_ratings.csv", col_types=cols(noteId=col_character()))
grok_output <- read_csv("data/valid_grok_ratings_560.csv", col_types=cols(noteId=col_character()))
```


```{r}
# check our notes and ratings match exactly 
setdiff(notes[["noteId"]], ratings[["noteId"]])
```

Ratings include our topic markers - so we'll mark the notes that have grok outputs 

```{r}
notes <- notes %>% mutate(has_grok_output = (noteId %in% (grok_output %>% pull(noteId)))) 

print(notes %>% filter(has_grok_output, (topic=="1_Ukraine"))) 
print(notes %>% filter(has_grok_output, (topic=="2_Gaza"))) 

```

We have 965 LLM notes that were assigned to our two conflict topics (Gaza and Ukraine). 

We can consider a few different ways of using the range of ratings for each individual note. Within X's Algorithm this is done using Bridge Based Matrix Factorization. This is somewhat beyond the scope of our project - though we would have loved to use it in comparing the LLM outputs. 

Our rough solution to having multiple ratings for one ground truth (whether or not the `lockedStatus` is marked as `CURRENTLY_RATED_HELPFUL` or `CURRENTLY_RATED_NOT_HELPFUL`). Is to take the mean of the ratings metrics (e.g. `notHelpfulOpinionSpeculation`). 

```{r}
grok_clean <- ready_grok(grok_output)
rated_notes <- ready_human_notes_ratings(notes=notes, ratings=ratings)
```


Though we have the notes metadata (i.e. what the note writer thinks of the tweet) this should be not considered directly as predicting locked status. Our hope more with including these is to give the models an understanding of consensus between note makers and note raters. 

We end up with only binary variables from the note data. 

Actually having the ratings be means and the LLM output let's us think about the nuance of opinion that a range of raters let's us have. 

# Modeling 


We'll start with a very simple model to check that helpfulness tracks to a positive `LockedStatus`. 

```{r}
base_model <- glm(data=rated_notes, locked_helpful ~ helpfulnessScore, family="binomial")
summary(base_model)
```


```{r}
modeling_data <- get_llm_note_ids(grok_output, rated_notes)
non_grok_rated_notes <- modeling_data$non_grok_rated_notes
grok_rated_notes <- modeling_data$grok_rated_notes
  

fit_log_rf <- function(notes_data){
  if(isFALSE(tibble::is_tibble(notes_data))){
    stop("restaurant_data should be a tibble")
  }
  set.seed(123) # For reproducibility

  train_ratio <- 0.7 
  smp_size <- floor(train_ratio * nrow(notes_data))
  
  train_indices <- sample(seq_len(nrow(notes_data)), size = smp_size)
  
  train <- notes_data[train_indices, ] %>% dplyr::select(locked_helpful, trustworthySources, all_of(rating_vars)) 
  test <- notes_data[-train_indices, ] %>% dplyr::select(locked_helpful, trustworthySources, all_of(rating_vars))

  outcomes <- test$locked_helpful
  
  # logistic regression  
  fit.lm <- glm(locked_helpful ~ ., data=train, family="binomial")
  logistic_predictions <- predict(fit.lm, newdata = test, type = "response")
  rocr_prediction_test <- ROCR::prediction(logistic_predictions, outcomes)
  auc_logistic <- c(performance(rocr_prediction_test, "auc")@y.values[[1]])
  
  # random forest
  fit.rf <- ranger(locked_helpful ~ .,
                   data = train, 
                   num.trees = 1000, 
                   respect.unordered.factors = TRUE, 
                   probability = TRUE)
  
  rf_pred <- predict(fit.rf, data = test, type="response")
  rf_predictions <- rf_pred$predictions[,2]
  
  rocr_prediction_test <- ROCR::prediction(rf_predictions, outcomes)
  auc_rf <- c(performance(rocr_prediction_test, "auc")@y.values[[1]])
  
  # output 
  output <- list(outcomes, logistic_predictions, rf_predictions, auc_logistic, auc_rf, fit.lm, fit.rf)
  names(output) <- c("outcomes", "logistic_predictions", "rf_predictions", "auc_logistic", "auc_rf", "log_model", "rf_model")
  
  return(output)

}

model_outputs <- fit_log_rf(non_grok_rated_notes)



```

As the model performs so well on all topics, there are likely better strategies to compare the LLM outputs to the human rated outputs. Below we will look at the average difference between the every. We'll do this with the meaned ratings as well as with the meaned ratings binarized. 

Essentially for every rating type we will find the difference between the LLM ratings and the human rating, sum this for the whole row (so we have a total differnce). 

As a note, though the ratings data is tree structured, we will include all variables. 

```{r}

combined_ratings <- left_join(grok_rated_notes, grok_clean, by="noteId", suffix=c(".human", ".grok"))

# get our 10 comparable rating values
helpfulness_names <- names(combined_ratings) %>%
  grep("\\.human$", ., value = TRUE) %>%
  gsub("\\.human$", "", .)


rating_diffs <- combined_ratings %>%
  # get values that start with the base names
  dplyr::select(starts_with(base_names), noteId) %>% 
  # 
  mutate(across(
    .cols = all_of(paste0(base_names, ".human")), 
    .fns = ~ . - get(gsub("\\.human$", ".grok", cur_column())), 
    .names = "diff.{gsub('.human$', '', col)}" 
  ),
  sum_o_squares = rowSums(across(all_of(starts_with("diff")), ~ .^2))) %>% 
  
  # Select only the newly created difference columns
  dplyr::select(starts_with("diff."), noteId, sum_o_squares)

```


We don't want to average out these differences, so we square and then sum them. 





Both of these models do really really well. This is great, though not unexpected. We will now compare our llm outputs! 








