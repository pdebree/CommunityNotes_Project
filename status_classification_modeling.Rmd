---
title: "status_classification_modeling.Rmd"
output: html_document
date: "2025-12-05"
---

```{r}
library(tidyverse)
library(ranger)
library(ROCR)
source("classification.R")
```

Read in the valid notes data, with topic markers, the ratings data and the generated LLM outputs

```{r}
notes <- read_csv("data/topic_valid_notes.csv", col_types=cols(noteId=col_character()))
ratings <- read_csv("data/valid_ratings.csv", col_types=cols(noteId=col_character()))
grok_output <- read_csv("data/valid_grok_ratings_560.csv", col_types=cols(noteId=col_character()))

# check our notes and ratings match exactly 
setdiff(notes[["noteId"]], ratings[["noteId"]])
```

Ratings include our topic markers - so we'll mark the notes that have grok outputs 

```{r}
# mark notes that have a corresponding grok output
notes <- notes %>% mutate(has_grok_output = (noteId %in% (grok_output %>% pull(noteId)))) 

print(notes %>% filter(has_grok_output, (topic=="1_Ukraine"))) 
print(notes %>% filter(has_grok_output, (topic=="2_Gaza")) %>% dplyr::select(starts_with(note_text)))

```

We have 965 LLM notes that were assigned to our two conflict topics (Gaza and Ukraine). 

We can consider a few different ways of using the range of ratings for each individual note. Within X's Algorithm this is done using Bridge Based Matrix Factorization. This is somewhat beyond the scope of our project - though we would have loved to use it in comparing the LLM outputs. 

Our rough solution to having multiple ratings for one ground truth (whether or not the `lockedStatus` is marked as `CURRENTLY_RATED_HELPFUL` or `CURRENTLY_RATED_NOT_HELPFUL`). Is to take the mean of the ratings metrics (e.g. `notHelpfulOpinionSpeculation`). 

```{r}
grok_output_clean <- ready_grok(grok_output, notes=notes)
rated_notes <- ready_human_notes_ratings(notes=notes, ratings=ratings) 
# version of the data frame where all the mean ratings were binarized.
rated_notes_binned <- rated_notes %>% mutate(across(all_of(rating_vars[-length(rating_vr)]),.fns = ~ as.numeric(. >= 0.5)))
```


Though we have the notes metadata (i.e. what the note writer thinks of the tweet) this should be not considered directly as predicting locked status. Our hope more with including these is to give the models an understanding of consensus between note makers and note raters. 


# Modeling 

We'll start with a very simple model to check that helpfulness tracks to a positive `LockedStatus`. 

```{r}
# check how effective helpfulness is at predicting lockedStatus 
base_model <- glm(data=rated_notes, locked_helpful ~ helpfulnessScore, family="binomial")
summary(base_model)
```

Next, we split our data into notes that have corresponding grok outputs and notes that do not. We train the model on only notes that do not have corresponding outputs, so that we can comparing the performance of the grok ratings with the human ratings. 

```{r}
modeling_data <- get_llm_note_ids(grok_output, rated_notes_binned)
non_grok_rated_notes <- modeling_data$non_grok_rated_notes
human_rated_notes_corr_grok <- modeling_data$grok_rated_notes
  
# run the function that fits the logitic regression and random forest model, returning the models and 
# evaluation metrics.
model_outputs <- fit_log_rf(non_grok_rated_notes)
```

As the model performs so well on all topics, there are likely better strategies to compare the LLM outputs to the human rated outputs. Below we will look at the average difference between the every. We'll do this with the meaned ratings as well as with the meaned ratings binarized.

Essentially for every rating type we will find the difference between the LLM ratings and the human rating, sum this for the whole row (so we have a total difference). 

As a note, though the ratings data is tree structured, we will include all variables. 

```{r}
# Create predictions for grok data
grok_output_clean_labs <- left_join(grok_output_clean, human_rated_notes_corr_grok %>% dplyr::select(noteId, locked_helpful), by="noteId")

logistic_predictions_grok <- predict(model_outputs$log_model, newdata = grok_output_clean_labs, type = "response")
rocr_prediction_test_grok <- ROCR::prediction(logistic_predictions_grok, grok_output_clean_labs$locked_helpful)
auc_logistic_grok <- c(performance(rocr_prediction_test_grok, "auc")@y.values[[1]])


logistic_predictions_humangrok <- predict(model_outputs$log_model, newdata = human_rated_notes_corr_grok, type = "response")
rocr_prediction_test_humangrok <- ROCR::prediction(logistic_predictions_humangrok, human_rated_notes_corr_grok$locked_helpful)
auc_logistic_humangrok <- c(performance(rocr_prediction_test_humangrok, "auc")@y.values[[1]])


rf_pred_grok <- predict(model_outputs$rf_model, data = grok_output_clean_labs, type="response")
rf_predictions_grok  <- rf_pred_grok $predictions[,1]

rocr_prediction_test_grok <- ROCR::prediction(rf_predictions_grok, grok_output_clean_labs$locked_helpful)
auc_rf_grok <- c(performance(rocr_prediction_test_grok, "auc")@y.values[[1]])

rf_pred_humangrok <- predict(model_outputs$rf_model, data = human_rated_notes_corr_grok, type="response")
rf_predictions_humangrok  <- rf_pred_humangrok$predictions[,1]

rocr_prediction_test_humangrok <- ROCR::prediction(rf_predictions_humangrok, human_rated_notes_corr_grok$locked_helpful)
auc_rf_humangrok <- c(performance(rocr_prediction_test_humangrok, "auc")@y.values[[1]])


```



# Bespoke RMSE Metric 

This bespoke difference metric calculates the difference in grok rating against human rating for all rating elements in one single row. Then these are summed and squared. This gives us a measure of the discrepancy between the two types of ratings, which we can compare across different groups. Specifically, we compare how the Ukraine conflict ratings compare to the Gaza conflict ratings.

```{r}
combined_ratings <- left_join(grok_rated_notes, grok_clean, by="noteId", suffix=c(".human", ".grok"))

# get our 10 comparable rating values
helpfulness_names <- names(combined_ratings) %>%
  grep("\\.human$", ., value = TRUE) %>%
  gsub("\\.human$", "", .)

non_bin_diffs <- find_diffs(combined_ratings)
non_bin_diffs %>% group_by(topic) %>% summarize(mean_eval_score=mean(sum_o_squares))

binned_combined_ratings <- combined_ratings %>% mutate(across(all_of(paste0(helpfulness_names, ".human")),.fns = ~ as.numeric(. >= 0.5)))

bin_diffs <- find_diffs(binned_combined_ratings)

bin_diffs %>% group_by(topic) %>% summarize(mean_eval_score=mean(sum_o_squares))
```






