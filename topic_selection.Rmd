---
title: "topic_selection"
output: html_document
date: "2025-12-03"
---

```{r}
library(tidyverse)
# For language input
library(ellmer)
library(cld2)
library(stringr)
library(openai)
library(data.table)
library(stm)
library(MASS)
library(tm)
library(topicmodels)
library(tidytext)
library(keyATM)
library(quanteda)
```


# Topic Modeling 

This notebook takes us through the process of creating defined topics for our Community Notes Project. Our idea in focusing on topics is to try to create distinct groups for the topics highlighted within X's code base (https://github.com/twitter/communitynotes/blob/main/scoring/src/scoring/topic_model.py). Our thinking in doing this is to find topics that are explicitly important to X - either because they are very sensitive subjects or because their values need more monitoring than others.

Though X has not launched per-topic modeling yet - it does intend to evaluate this method ("This approach represents a prelimiary approach to topic assignment while Community Notes evaluates the efficacy of per-topic note scoring." - line 8 of the `topic_model.py` file linked above). 

There are four topics that are explicitly called our by X in it's code base. This is done by having seed terms defined for these specific topics and then using these seed terms in the topic modeling. Though X does not limit topics to these groups, we were very interested in why they would declare seed terms for these groups specifically. 

The topics with seed terms are as follows: 
- `UkraineConflict`
- `GazaConflict`
- `MessiRonaldo`
- `Scams`

Intuitively, it makes sense that the first two topics - the conflict topics - are highlighted. X would want to be sensitive to how Community Notes responds to these areas, given the differences of opinions and mass misinformation spread about these conflicts. 

We aren't quite sure why the question of the Greatest Footballer of All Time debate is included in these Seed Terms, but maybe it represents an area where opinion blurs fact. For example, imagine a tweet that claims Messi to be the best football player, somebody could create a note claiming that this is not true and that Ronaldo is better. This would not be a valid Community Note, but we could understand the logic of needing to ensure these types of notes are handled. 

Finally, it is likely very important for X to minimize the amount of scams and misinformation that are spread on it's platform. For this reason, it makes sense that they target language used in addressing scams to safeguard the online community. 

We should note, that these topics are generally applied to the tweet text as well as the notes text. (In X's code base they combine these two to do the topic modelling). Because we do not have access to the tweet data, our topic modeling is done only on the note text. This is a limitation as you could imagine a note that has a tone like this: "This is False, the war has not ended *link to source*". This is not specific enough to be bucketed into one conflict. Also a major limitation of topic modeling is that it loses the sentiment or topics that come from the linked urls. In the example tweet above, the conflict could be specified in the link - but regular language based topic modeling would not capture this. 


```{r}
# Only need valid notes here, ratings are not relevant to our topics. (They will be once we start modeling). 
valid_notes <- read_csv("data/valid_notes.csv", col_types=cols(noteId=col_character()))
```

The functions for our topic modeling process can be found in `topic_modeling.R`. Below in the `mark_topics` function we create binary variable that indicate whether a note contains a seed term for each of the four topics. We will use this to check how well our topic model does at highlighting notes that should be within a group.

```{r}
source("topic_modeling.R")

# add seed term markers to our valid notes 
valid_notes <- mark_topics(valid_notes)
topic_names <- c("ukraine_conflict", "gaza_conflict", "messi_ronaldo", "scams") 

# Checking how many of each topic are seen by the values 
colSums(valid_notes[topic_names])
```

The seed topics appear in the following number of notes: 
- `UkraineConflict` - 4319 
- `GazaConflict` - 6349
- `MessiRonaldo` - 453
- `Scams` - 6575
            
            
```{r}
# We can limit our data set to only those that include the seed terms (in an earlier phase we did this to see how well the topic modeling would perform with - ideally - very strict groups.)
topic_only_valid <- limit_to_four_topics(valid_notes)
gaza_ukraine_valid <- limit_gaza_ukraine(valid_notes)

#write.csv(gaza_ukraine_valid, "data/gaza_ukraine_notes.csv", row.names=FALSE)
```

This is a really rough way of getting the data - we could use these seed terms to make more nuanced topic modeling. For our analysis we believe that finding a small subset is enough (especially given the constraint on our data). 


# KeyATM 
[https://keyatm.github.io/keyATM/index.html] - Add proper reference. Write this up properly. 

As we have seed terms for our Topic Modelings (we are not doing vanilla LDA), we looked for an algorithm for Topic Modeling that uses seed terms to aid in the creation of topics. We decided on KeyATM () - Give justification and explaination 

KeyATM is very integrated with the `quanteda` package - which we will use to help do our text pre-processing for the actual topic modeling algorithm. In particular the `tokens` function is useful in creating tokens from word processing. We also use the `stopwords` function from this package, with `english` as the parameter, to get a list of common words that are typically removed in text processing. 

Below we create the list of notes text (order here will be very important). 

```{r}

# Pull out the full vector of notes for all of our valid English tweets ()
full_notes_text <- valid_notes[["note_text"]]

text_preprocessing <- function(notes_text) {
  
  custom_stopwords <- c(
    "quote", "video", "account", "accounts", "post", "claim", "use", "quotes", "image")
  
  # Create tokens  - could look at making sub-words?
  data_tokens <- quanteda::tokens(
  full_notes_text,
  # Preprocessing Steps 
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE, 
  remove_url = TRUE
) %>%
  # make all tokens lower case
  tokens_tolower() %>%
  # removes list of stop words imported from the quanteda package. 
  tokens_remove(
    c(
      quanteda::stopwords("english"), custom_stopwords
    )
  ) %>%
  # stem words
  tokens_wordstem(
    language = quanteda_options("language_stemmer"),
    verbose = quanteda_options("verbose")
  ) %>% 
  # removes tokens of length 2 or less
  tokens_select(min_nchar = 3) 

  # word_counts_vector <- colSums(data_dfm)
  # head(sort(word_counts_vector, decreasing = TRUE), 200)
  
  data_dfm <- dfm(data_tokens) %>%
    # min_termfreq - removes features that appear less than five times across all documents 
    # min_docfreq - removes features that appear in less than 2 distinct documents. 
    # get 4200 from looking at the data. 
    dfm_trim(min_termfreq = 5, min_docfreq = 2, max_termfreq = 3000)
  
  # find indices for rows that are not empty
  logical_empty <- unname(rowSums(data_dfm) > 0)
  
  
  # Subset dfm to only include those that do not have empty rows. 
  data_dfm_filtered <- dfm_subset(
      data_dfm,
      logical_empty
  )

  docs <- keyATM_read(
  texts = data_dfm_filtered)

  output <- list(docs=docs, logical_empty=logical_empty)
  return(output)

}

# Preprocess docs to prepare for topic modeling.
preprocessed_docs <- text_preprocessing(full_notes_text)
```

Once we have a list of tokens, we can then create our Document Term Frequency Matrix, using `quanteda::dfm`. We pass our data tokens to this function and remove terms that appear less than 5 times across all documents and those that appear in less than 2 distinct documents.

Filtering takes us from 138894 to 138473 rows. 

Below we do some visualization of the words. We do this for all the seed terms as well as just the conflict topics. 

```{r}
# We visualize the keyword frequency 
key_viz <- visualize_keywords(docs = preprocessed_docs$docs, keywords = keywords)
key_viz

# We can also visualize the keyword frequency for just the conflict topics
key_viz_gzuk <- visualize_keywords(docs = preprocessed_docs$docs, keywords = keywords_gzuk)
key_viz_gzuk 

# Visualize the words frequency. 
values_fig(key_viz)
```

# A tibble: 16 × 5
# Groups:   Topic [4]
   Word      WordCount `Proportion(%)` Ranking Topic    
   <chr>         <int>           <dbl>   <int> <fct>    
 1 ukrain         1915           0.14        1 1_Ukraine
 2 russia         1609           0.117       2 1_Ukraine
 3 putin           403           0.029       3 1_Ukraine
 4 moscow          113           0.008       4 1_Ukraine
 5 kyiv            101           0.007       5 1_Ukraine
 6 kiev             41           0.003       6 1_Ukraine
 7 israel         2614           0.191       1 2_Gaza   
 8 gaza           1264           0.092       2 2_Gaza   
 9 hama           1191           0.087       3 2_Gaza   
10 palestin        643           0.047       4 2_Gaza   
11 jerusalem        98           0.007       5 2_Gaza   
12 messi           242           0.018       1 3_Messi  
13 ronaldo         222           0.016       2 3_Messi  
14 term           2604           0.19        1 4_Scams  
15 farm           1980           0.144       2 4_Scams  
16 dropship       1161           0.085       3 4_Scams  


Ready text for modeling with keyATM_read (creates a keyATM_docs object). 

We compare the keyATM with a weightedLDA to see how much adding the keywords improves our topics discrepancies. Looking at the weightedLDA output also let's us see which words appear across the 

```{r}
out_lda <- weightedLDA(
  docs              = preprocessed_docs$docs, 
  number_of_topics  = 10, # We do 10 to match the logic below
  model             = "base",
  options           = list(seed = 250)
)

# prints the top words for each of the 10 topics. 
top_words(out_lda) 
```

We use this output to look at words that appear across the groups, and add them to our custom stop words list. 

   Topic_1  Topic_2     Topic_3  Topic_4  Topic_5  Topic_6  Topic_7 Topic_8 Topic_9 Topic_10
1  confirm advertis    dropship    tesla imperson dropship   vaccin  ukrain  israel   vaccin
2  support     site    platform imperson    steal    legal     caus  russia  attack     caus
3    satir     rule  aliexpress   verifi     warn     call    water  israel  ukrain  opinion
4     vote     term       unfit    steal   @tesla platform imperson  attack  presid  support
5     rule    stake       legal    repli    tesla    taken    20the russian     war     call
6      new  without      outlet    asset   verifi    satir contrail     old  govern   govern
7    first prohibit        lack   @tesla    asset    digit     call     war support    found
8     term   israel      safeti   letter     elon    first    drive confirm opinion      new
9   presid   provid      market     fool   andrew  confirm    creat   first  russia   israel
10 opinion  generat counterfeit  misread   crypto     lack     mani    part    rule  confirm


This does look far better than before we had our top topics. 

From this output we see that there are not very coherent groups - this makes intuitive sense as there is a very, very large range of potential topics within this data set. Generally, we have groups that have a lot of overlap as well. There is not a clear groups and there is a lot of overlap between the groups. This may be because there simply aren't enough topics with distinct values. You could also argue that there are domain specific stop words that should be removed like "video", "post", "account" etc that refer to the logistics of having a notes be about a tweet. 

Now let's look at keyATM to do Topic Modeling with Seed Terms. 

```{r}
out_keyATM <- keyATM(
  docs              = preprocessed_docs$docs, 
  no_keyword_topics = 10, # 4 plus 6 extra to account for differences in text data. 
  keywords          = keywords, # selected keywords
  model             = "base", # select the model
  options           = list(seed = 250)
)

top_words(out_keyATM)
```

 1_Ukraine      2_Gaza 3_Messi      4_Scams Other_1  Other_2  Other_3  Other_4 Other_5 Other_6 Other_7 Other_8
1  ukrain [✓]  israel [✓]    game        gambl    musk    water   vaccin     musk opinion     tax   20the     law
2  russia [✓]    gaza [✓]   leagu    undisclos   steal     game     caus    steal   alter   still    text  presid
3     russian    hama [✓]    goal     term [✓]   asset    creat   cancer   verifi    musk current    work opinion
4         war        isra  player     advertis  @tesla      see    studi imperson   satir    sinc     new   charg
5     countri      attack  season       polici  verifi     caus    human   public    news  vaccin    20of    vote
6      presid palestinian    play        stake  public   differ     anim    asset    elon  provid    vote   court
7   ukrainian        kill     won      product   tesla contrail    water   @tesla  pictur     new    game    case
8    militari         war   score     platform  crypto  generat   health     warn generat confirm opinion   offic
9      attack         jew   first dropship [✓] compani      new scientif     elon   digit opinion current current
10     govern     support   alter       websit scammer  opinion     mani  compani    call    news  public     man
   Other_9 Other_10
1   pictur imperson
2     game   letter
3      day     fool
4  confirm  misread
5   releas  opinion
6      see      it’
7     fire    alter
8    first     news
9      new    satir
10     it’   presid

The output shows that there are fairly good groups for our selected seed topics.

```{r}
fig_modelfit <- plot_modelfit(out_keyATM)
fig_modelfit
```


Now we can assign these topics to our original model 
```{r}

assign_topics <- function(notes_frame, keyATM_output, non_empty_logical) {
  
  
  # pull out the proportions of appearance in each topic group (out of the 10)
  theta_matrix <- keyATM_output$theta
  
  # Find the column in which the highest likelihood topic appears. 
  highest_prop_topic <- apply(theta_matrix, 1, which.max)
  
  # pull out topic names (this let's us see our seed topics)
  topic_labels <- colnames(theta_matrix)
  
  # assign indices only to documents that were used in the topic modeling 
  # remember we omitted some!
  assigned_topics_name_filtered <- topic_labels[highest_prop_topic]
  
  
  non_empty_doc_indices <- which(non_empty_logical)

  # create column of NA values 
  notes_frame$keyATM_topic <- NA
  
  # Assign the topic group the documents (based on their indices)
  notes_frame[non_empty_doc_indices, "keyATM_topic"] <- assigned_topics_name_filtered
  
  return(notes_frame)
}


topic_notes_valid <- assign_topics(
  notes_frame = valid_notes, keyATM_output = out_keyATM, non_empty_logical = preprocessed_docs$logical_empty) 

# only keep interpretability for seed topics. 
topic_notes_valid <- topic_notes_valid %>% mutate(
  topic=ifelse(keyATM_topic %in% c("1_Ukraine", "2_Gaza", "3_Messi", "4_Scams"), keyATM_topic, "unassigned"))
  
table(topic_notes_valid$topic)
```


 1_Ukraine     2_Gaza    3_Messi    4_Scams unassigned 
     10581      13295       8904       7949      83618 


Now it is useful to check whether the notes that have been assigned based on just the seed terms end up in the right groups. Of course this is a fairly rough test but we can get some intuition on whether the seed terms track well to the ultimate topic groups. 

```{r}
print(topic_notes_valid %>% filter(ukraine_conflict) %>% summarize(ukraine_assigned = sum(topic == "1_Ukraine", na.rm = TRUE)/n()))
print(topic_notes_valid %>% filter(gaza_conflict) %>% summarize(gaza_assigned = sum(topic == "2_Gaza", na.rm = TRUE)/n()))
print(topic_notes_valid %>% filter(messi_ronaldo) %>% summarize(messi_assigned = sum(topic == "3_Messi", na.rm = TRUE)/n()))
print(topic_notes_valid %>% filter(scams) %>% summarize(scams_assigned = sum(topic == "4_Scams", na.rm = TRUE)/n()))
```
uk - 0.719, gz - 0.732, mr -  0.819, scams - 0.468


We'll check if this problem is coming from the NA values (in other words documents that did not have enough nuance for our topic modeling)

```{r}
print(topic_notes_valid %>% filter(ukraine_conflict) %>% summarize(num_na = sum(is.na(keyATM_topic))))
print(topic_notes_valid %>% filter(gaza_conflict) %>% summarize(num_na = sum(is.na(keyATM_topic))))
print(topic_notes_valid %>% filter(messi_ronaldo) %>% summarize(num_na = sum(is.na(keyATM_topic))))
print(topic_notes_valid %>% filter(scams) %>% summarize(num_na = sum(is.na(keyATM_topic))))
```

19 - 26 - 1 - 4 

It does not come from this, which is interesting. We'll save this updated dataset, so that we can used the topic allocation later 

Justification for only Gaza and Ukraine

```{r}
write.csv(topic_notes_valid, "data/topic_valid_notes.csv", row.names = FALSE)
```

